{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import betfairlightweight\n",
    "from betfairlightweight import filters\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from betfair import Betfair\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to import daily data so that we can get the race ids and loop through for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_key = 'ABGJLOlKaLtTsMIp'\n",
    "\n",
    "username1 = input(\"Email:\")\n",
    "password1 = input(\"Password:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading = betfairlightweight.APIClient(\n",
    "        username1, password1, app_key\n",
    "    )\n",
    "trading.login_interactive()\n",
    "\n",
    "trading.session_token\n",
    "\n",
    "trading.keep_alive()\n",
    "\n",
    "trading.race_card.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '../data/daily'\n",
    "destination_path = '../data/processed_daily'\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "csv_files = [f for f in os.listdir(source_path) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024-08-22_data.csv',\n",
       " '2024-08-23_data.csv',\n",
       " '2024-08-16_data.csv',\n",
       " '2024-08-13_data.csv',\n",
       " '2024-08-26_data.csv',\n",
       " '2024-08-15_data.csv',\n",
       " '2024-08-14_data.csv']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_race_results(market_ids):\n",
    "    try:\n",
    "        data = trading.race_card.get_race_result(market_ids=market_ids)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        data = []\n",
    "\n",
    "    # Process and display the data\n",
    "    flat_data = []\n",
    "\n",
    "    for race in data:\n",
    "        for runner in race.get('runners', []):\n",
    "            for selection in runner.get('selections', []):\n",
    "                if selection['marketType'] == 'WIN' and 'bsp' in selection:\n",
    "                    flat_data.append({\n",
    "                        'race_id': race.get('raceId'),\n",
    "                        'country_code': race.get('course', {}).get('countryCode'),\n",
    "                        'race_title': race.get('raceTitle'),\n",
    "                        'race_class': race.get('raceClassification', {}).get('classification'),\n",
    "                        'distance': race.get('distance'),\n",
    "                        'course_type': race.get('course', {}).get('courseType'),\n",
    "                        'surface_type': race.get('course', {}).get('surfaceType'),\n",
    "                        'market_id': selection.get('marketId'),\n",
    "                        'horse_id': runner.get('horseId'),\n",
    "                        'saddle_cloth': runner.get('saddleCloth'),\n",
    "                        'isNonRunner': runner.get('isNonRunner'),\n",
    "                        'position': runner.get('position'),\n",
    "                        'selection_id': selection.get('selectionId'),\n",
    "                        'bsp': selection.get('bsp')\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    df = pd.read_csv(os.path.join(source_path, csv))\n",
    "\n",
    "    # Convert columns to string\n",
    "    df['market_id'] = df['market_id'].astype(str)\n",
    "    df['selection_id'] = df['selection_id'].astype(str)\n",
    "\n",
    "    market_ids = df['market_id'].unique()\n",
    "\n",
    "    results_df = fetch_race_results(market_ids)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(f\"No data returned for market_ids: {market_ids}. Skipping this file.\")\n",
    "        continue  # Skip if no data is returned\n",
    "    \n",
    "    # Check for missing columns\n",
    "    if 'market_id' not in results_df.columns or 'selection_id' not in results_df.columns:\n",
    "        print(f\"Expected columns missing in results. Available columns: {results_df.columns}\")\n",
    "        continue  # Skip if required columns are missing\n",
    "    \n",
    "    # Convert columns to string\n",
    "    results_df['market_id'] = results_df['market_id'].astype(str)\n",
    "    results_df['selection_id'] = results_df['selection_id'].astype(str)\n",
    "\n",
    "    df_merged = df.merge(results_df, left_on=['market_id', 'selection_id'], right_on=['market_id', 'selection_id'], how='left')\n",
    "\n",
    "    merged_dfs.append(df_merged)\n",
    "\n",
    "    processed_file_path = os.path.join(destination_path, csv)\n",
    "    df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Concatenate all merged DataFrames\n",
    "final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "\n",
    "final_df = final_df[final_df['status'].str.contains('ACTIVE', case=False)]\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv('../data/results/results.csv', index=False)\n",
    "\n",
    "os.remove(source_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleared contents of: ../data/daily\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_directory_contents(directory_path):\n",
    "    try:\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file or link\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove the directory\n",
    "        print(f\"Successfully cleared contents of: {directory_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while clearing directory: {e}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = '../data/daily'\n",
    "delete_directory_contents(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
